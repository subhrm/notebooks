{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Coursera-TF-Course-2-w2-Cat-vs-Dogs.ipynb","provenance":[{"file_id":"https://github.com/lmoroney/dlaicourse/blob/master/Exercises/Exercise%206%20-%20Cats%20v%20Dogs%20with%20Augmentation/Exercise%206%20-%20Question.ipynb","timestamp":1597168743125}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"zX4Kg8DUTKWO","colab_type":"code","colab":{}},"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7td5-VossQ-y","colab_type":"text"},"source":[" Test with [Complex examples](https://pixabay.com/photos/bed-dog-animals-pets-relax-1284238/)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dn-6c02VmqiN","colab":{}},"source":["# In this exercise you will train a CNN on the FULL Cats-v-dogs dataset\n","# This will require you doing a lot of data preprocessing because\n","# the dataset isn't split into training and validation for you\n","# This code block has all the required inputs\n","import os\n","import zipfile\n","import random\n","import tensorflow as tf\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from shutil import copyfile, rmtree"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3sd9dQWa23aj","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1597167048631,"user_tz":-330,"elapsed":52370,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"0c1707bc-2daf-486e-bb4a-85f21e471df4"},"source":["# This code block downloads the full Cats-v-Dogs dataset and stores it as \n","# cats-and-dogs.zip. It then unzips it to /tmp\n","# which will create a tmp/PetImages directory containing subdirectories\n","# called 'Cat' and 'Dog' (that's how the original researchers structured it)\n","# If the URL doesn't work, \n","# .   visit https://www.microsoft.com/en-us/download/confirmation.aspx?id=54765\n","# And right click on the 'Download Manually' link to get a new URL\n","\n","!wget --no-check-certificate \\\n","    \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\" \\\n","    -O \"/tmp/cats-and-dogs.zip\"\n","\n","local_zip = '/tmp/cats-and-dogs.zip'\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('/tmp')\n","zip_ref.close()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-08-11 17:30:00--  https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\n","Resolving download.microsoft.com (download.microsoft.com)... 23.52.161.19, 2600:1400:c000:3a8::e59, 2600:1400:c000:39f::e59\n","Connecting to download.microsoft.com (download.microsoft.com)|23.52.161.19|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 824894548 (787M) [application/octet-stream]\n","Saving to: ‘/tmp/cats-and-dogs.zip’\n","\n","/tmp/cats-and-dogs. 100%[===================>] 786.68M  28.4MB/s    in 36s     \n","\n","2020-08-11 17:30:36 (22.1 MB/s) - ‘/tmp/cats-and-dogs.zip’ saved [824894548/824894548]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gi3yD62a6X3S","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1597167069984,"user_tz":-330,"elapsed":1066,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"537fb009-6dda-48b6-a5cf-7caeb996b02f"},"source":["print(len(os.listdir('/tmp/PetImages/Cat/')))\n","print(len(os.listdir('/tmp/PetImages/Dog/')))\n","\n","# Expected Output:\n","# 12501\n","# 12501"],"execution_count":null,"outputs":[{"output_type":"stream","text":["12501\n","12501\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"F-QkLjxpmyK2","colab":{}},"source":["# Use os.mkdir to create your directories\n","# You will need a directory for cats-v-dogs, and subdirectories for training\n","# and testing. These in turn will need subdirectories for 'cats' and 'dogs'\n","try:\n","    #YOUR CODE GOES HERE\n","    os.makedirs(\"/tmp/cats-v-dogs/training/cats\", exist_ok=True)\n","    os.makedirs(\"/tmp/cats-v-dogs/training/dogs\", exist_ok=True)\n","    os.makedirs(\"/tmp/cats-v-dogs/testing/cats\", exist_ok=True)\n","    os.makedirs(\"/tmp/cats-v-dogs/testing/dogs\", exist_ok=True)\n","except OSError:\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zvSODo0f9LaU","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1597167724069,"user_tz":-330,"elapsed":6736,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"76f931e3-9226-4046-8e05-66a4385f7869"},"source":["# Write a python function called split_data which takes\n","# a SOURCE directory containing the files\n","# a TRAINING directory that a portion of the files will be copied to\n","# a TESTING directory that a portion of the files will be copie to\n","# a SPLIT SIZE to determine the portion\n","# The files should also be randomized, so that the training set is a random\n","# X% of the files, and the test set is the remaining files\n","# SO, for example, if SOURCE is PetImages/Cat, and SPLIT SIZE is .9\n","# Then 90% of the images in PetImages/Cat will be copied to the TRAINING dir\n","# and 10% of the images will be copied to the TESTING dir\n","# Also -- All images should be checked, and if they have a zero file length,\n","# they will not be copied over\n","#\n","# os.listdir(DIRECTORY) gives you a listing of the contents of that directory\n","# os.path.getsize(PATH) gives you the size of the file\n","# copyfile(source, destination) copies a file from source to destination\n","# random.sample(list, len(list)) shuffles a list\n","\n","def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n","# YOUR CODE STARTS HERE\n","\n","    # empty directories\n","    rmtree(TRAINING)\n","    rmtree(TESTING)\n","    \n","    # Recreate directories\n","    os.makedirs(TRAINING, exist_ok=True)\n","    os.makedirs(TESTING, exist_ok=True)\n","    \n","    # Note the contents \n","    print(f\"Num of files in TRAINING : {len(os.listdir(TRAINING))}\")\n","    print(f\"Num of files in TESTING : {len(os.listdir(TESTING))}\")\n","\n","    original_files = list(os.listdir(SOURCE))\n","    train_files = random.sample(original_files, int(len(original_files)*SPLIT_SIZE))\n","    test_set = set(original_files).difference(set(train_files))\n","    test_files = list(test_set)\n","    \n","    for f in train_files:\n","        source_path = os.path.join(SOURCE, f)\n","        if os.stat(source_path).st_size > 0:\n","            copyfile(source_path, os.path.join(TRAINING,f))\n","        else:\n","            print(f\"{source_path} is empty\")\n","        \n","    for f in test_files:\n","        source_path = os.path.join(SOURCE, f)\n","        if os.stat(source_path).st_size > 0:\n","            copyfile(source_path, os.path.join(TESTING,f))\n","        else:\n","            print(f\"{source_path} is empty\")\n","\n","# YOUR CODE ENDS HERE\n","\n","\n","CAT_SOURCE_DIR = \"/tmp/PetImages/Cat/\"\n","TRAINING_CATS_DIR = \"/tmp/cats-v-dogs/training/cats/\"\n","TESTING_CATS_DIR = \"/tmp/cats-v-dogs/testing/cats/\"\n","DOG_SOURCE_DIR = \"/tmp/PetImages/Dog/\"\n","TRAINING_DOGS_DIR = \"/tmp/cats-v-dogs/training/dogs/\"\n","TESTING_DOGS_DIR = \"/tmp/cats-v-dogs/testing/dogs/\"\n","\n","split_size = .9\n","split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size)\n","split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size)\n","\n","# Expected output\n","# 666.jpg is zero length, so ignoring\n","# 11702.jpg is zero length, so ignoring"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Num of files in TRAINING : 0\n","Num of files in TESTING : 0\n","/tmp/PetImages/Cat/666.jpg is empty\n","Num of files in TRAINING : 0\n","Num of files in TESTING : 0\n","/tmp/PetImages/Dog/11702.jpg is empty\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"luthalB76ufC","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1597167728551,"user_tz":-330,"elapsed":1212,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"827e53a5-2bd1-4669-d4d7-9d576a7e8992"},"source":["print(len(os.listdir('/tmp/cats-v-dogs/training/cats/')))\n","print(len(os.listdir('/tmp/cats-v-dogs/training/dogs/')))\n","print(len(os.listdir('/tmp/cats-v-dogs/testing/cats/')))\n","print(len(os.listdir('/tmp/cats-v-dogs/testing/dogs/')))\n","\n","# Expected output:\n","# 11250\n","# 11250\n","# 1250\n","# 1250"],"execution_count":null,"outputs":[{"output_type":"stream","text":["11249\n","11249\n","1251\n","1251\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-BQrav4anTmj","colab":{"base_uri":"https://localhost:8080/","height":527},"executionInfo":{"status":"ok","timestamp":1597168495025,"user_tz":-330,"elapsed":1106,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"05b792ea-4aa2-4b4e-fc82-5b1def357ca1"},"source":["# DEFINE A KERAS MODEL TO CLASSIFY CATS V DOGS\n","# USE AT LEAST 3 CONVOLUTION LAYERS\n","\n","tf.keras.backend.clear_session()\n","\n","model = tf.keras.models.Sequential([\n","# YOUR CODE HERE\n","    tf.keras.layers.Conv2D(32, (3,3), activation=\"relu\", input_shape=(150,150,3)),\n","    tf.keras.layers.MaxPooling2D((2,2)),\n","    tf.keras.layers.Conv2D(48, (3,3), activation=\"relu\"),\n","    tf.keras.layers.MaxPooling2D((2,2)),\n","    tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\"),\n","    tf.keras.layers.MaxPooling2D((2,2)),\n","    tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\"),\n","    tf.keras.layers.MaxPooling2D((2,2)),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(256,activation=\"relu\"),\n","    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n","])\n","\n","model.compile(optimizer=RMSprop(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 148, 148, 32)      896       \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 74, 74, 32)        0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 72, 72, 48)        13872     \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 36, 36, 48)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 34, 34, 64)        27712     \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 17, 17, 64)        0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 15, 15, 64)        36928     \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 7, 7, 64)          0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 3136)              0         \n","_________________________________________________________________\n","dense (Dense)                (None, 256)               803072    \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 257       \n","=================================================================\n","Total params: 882,737\n","Trainable params: 882,737\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mlNjoJ5D61N6","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1597168526566,"user_tz":-330,"elapsed":1489,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"349fcad3-c4ec-49c1-bf81-c470295aeab5"},"source":["TRAINING_DIR = \"/tmp/cats-v-dogs/training/\" #YOUR CODE HERE\n","train_datagen = ImageDataGenerator(rescale=1.0/255.0,\n","                                  rotation_range=0.2,\n","                                  shear_range=0.2,\n","                                  horizontal_flip=True,\n","                                  zoom_range=0.2,\n","                                  width_shift_range=0.2,\n","                                  height_shift_range=0.2)\n","\n","# NOTE: YOU MUST USE A BATCH SIZE OF 10 (batch_size=10) FOR THE \n","# TRAIN GENERATOR.\n","train_generator = train_datagen.flow_from_directory(\n","    TRAINING_DIR,\n","    target_size=(150,150),\n","    batch_size=100,\n","    class_mode=\"binary\"\n",")\n","\n","\n","VALIDATION_DIR = \"/tmp/cats-v-dogs/testing/\" #YOUR CODE HERE\n","validation_datagen = ImageDataGenerator(rescale=1.0/255.0) #YOUR CODE HERE\n","\n","# NOTE: YOU MUST USE A BACTH SIZE OF 10 (batch_size=10) FOR THE \n","# VALIDATION GENERATOR.\n","validation_generator = train_datagen.flow_from_directory(\n","    VALIDATION_DIR,\n","    target_size=(150,150),\n","    batch_size=100,\n","    class_mode=\"binary\"\n",")\n","\n","\n","\n","# Expected Output:\n","# Found 22498 images belonging to 2 classes.\n","# Found 2500 images belonging to 2 classes."],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 22496 images belonging to 2 classes.\n","Found 2502 images belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KyS4n53w7DxC","colab":{"base_uri":"https://localhost:8080/","height":343},"outputId":"00b4877d-f23a-4d3f-fe9b-60d2bb9bd822"},"source":["history = model.fit(train_generator,\n","                    epochs=10,\n","                    steps_per_epoch=22500//100,\n","                    verbose=1,\n","                    validation_data=validation_generator,\n","                    validation_steps=2500//100)\n","\n","# The expectation here is that the model will train, and that accuracy will be > 95% on both training and validation\n","# i.e. acc:A1 and val_acc:A2 will be visible, and both A1 and A2 will be > .9"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n"," 61/225 [=======>......................] - ETA: 2:13 - loss: 0.7098 - accuracy: 0.5259"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 32 bytes but only got 0. Skipping tag 270\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 5 bytes but only got 0. Skipping tag 271\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 272\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 282\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 283\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 20 bytes but only got 0. Skipping tag 306\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 48 bytes but only got 0. Skipping tag 532\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n","  warnings.warn(str(msg))\n"],"name":"stderr"},{"output_type":"stream","text":["225/225 [==============================] - ETA: 0s - loss: 0.6834 - accuracy: 0.5708"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MWZrJN4-65RC","colab":{}},"source":["# PLOT LOSS AND ACCURACY\n","%matplotlib inline\n","\n","import matplotlib.image  as mpimg\n","import matplotlib.pyplot as plt\n","\n","#-----------------------------------------------------------\n","# Retrieve a list of list results on training and test data\n","# sets for each training epoch\n","#-----------------------------------------------------------\n","acc=history.history['accuracy']\n","val_acc=history.history['val_accuracy']\n","loss=history.history['loss']\n","val_loss=history.history['val_loss']\n","\n","epochs=range(len(acc)) # Get number of epochs\n","\n","#------------------------------------------------\n","# Plot training and validation accuracy per epoch\n","#------------------------------------------------\n","plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n","plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n","plt.title('Training and validation accuracy')\n","plt.figure()\n","\n","#------------------------------------------------\n","# Plot training and validation loss per epoch\n","#------------------------------------------------\n","plt.plot(epochs, loss, 'r', \"Training Loss\")\n","plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n","\n","\n","plt.title('Training and validation loss')\n","\n","# Desired output. Charts with training and validation metrics. No crash :)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LqL6FYUrtXpf","colab":{}},"source":["# Here's a codeblock just for fun. You should be able to upload an image here \n","# and have it classified without crashing\n","\n","import numpy as np\n","from google.colab import files\n","from keras.preprocessing import image\n","\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n"," \n","  # predicting images\n","  path = '/content/' + fn\n","  img = image.load_img(path, target_size=(# YOUR CODE HERE))\n","  x = image.img_to_array(img)\n","  x = np.expand_dims(x, axis=0)\n","\n","  images = np.vstack([x])\n","  classes = model.predict(images, batch_size=10)\n","  print(classes[0])\n","  if classes[0]>0.5:\n","    print(fn + \" is a dog\")\n","  else:\n","    print(fn + \" is a cat\")"],"execution_count":null,"outputs":[]}]}